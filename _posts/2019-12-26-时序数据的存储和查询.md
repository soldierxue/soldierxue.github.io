---
layout:     post
title:      时序数据的存储和查询
subtitle:   到底是使用 NoSQL 还是传统的关系数据库来处理时序数据？
date:       2019-12-26
author:     薛以致用
catalog: true
tags:
    - 数据库
    - 时序
    - 最佳实践
---
# 前言

如今在万物互联（IoT）兴起的推动下，时间序列数据（衡量事物随时间变化的数据）应用和场景激增，是增长最快的数据类型之一，比如监控指标数据，传感器数据，日志，财务分析等等；时间序列数据具有特定的特征，例如通常以时间顺序形式出现，数据只能附加，并且查询总是在一个时间间隔内进行。虽然关系数据库可以存储这些数据，但是它们在处理这些数据时效率低下，因为它们缺乏优化，例如按时间间隔存储和检索数据。在这个[时序数据库项目列表页](https://misfra.me/2016/04/09/tsdb-list/)有超 50种方案，比如 Apache Kudu/Cassandra，ClickHouse, Druid, EasticSearch, HBase, InfluxDB,Prometheus,TimescaleDB，Amazon Redshift/Timestream(预览)，Google BigQuery，Facebook Scuba 等等；

很多上规模的客户在时序数据的摄入（Ingest)、分析和成本方面都会遇到很多挑战，本文我们先尝试理解下时序数据的特殊性和数据库选型问题。

# 时序数据怎么不一样？

关系数据库（OLTP）的出现是为了解决交易事务问题，比如电商订单，支付等场景，也就是数据表的事务更新，比如转账交易，用户从一个账号转出，而在另外一个账户入账；这对应数据库两行甚至两个字段的更新；由于任何两个账号之间都可能发生转账，因此这些记录在磁盘上随机分布；再让我们看看时序数据的场景：

- 虚拟机/容器/应用监控数据：系统通常会收集不同服务器、容器或应用的度量值，比如 CPU 利用率，可用内存，可用磁盘，网络传输字节总量，每秒请求数等等，每个指标都关联相关的时间戳，服务器 ID，和一组描述所收集内容的属性；

    {
    "name":       "server.requestCount",
    "status":     "200",
    "endpoint":   "api",
    "nf.app":     "fooserver",
    "nf.cluster": "fooserver-main",
    "nf.stack":   "main",
    "nf.region":  "us-east-1",
    "nf.zone":    "us-east-1c",
    "nf.node":    "i-12345678"
    }

- 传感器数据：每个设备可以在每个时间段报告多个传感器读数；例如对于空气和环境质量检测，可能包含，温度、湿度、气压、有害物质、颗粒物等等的测量值；每组数据都与时间戳、唯一设备ID相关联，并且可能有其他元数据。
- 证券行情数据：用时间戳的信息流表示，包含证券代码，当前价格，价格变化等等
- 车队/资产管理：数据包含车辆/资产ID，时间戳，GPS 坐标，及可能的元数据

以上所有的场景，**数据集都是连续的测量值，不断产生“新数据”插入到数据库**，虽然由于网络延迟，可能存在数据到达后端的时候，比数据生成标记的时间戳要晚很多，不过这种情况属于异常情况，发生频率比较少；

对比来看，OLTP事务处理的数据写入和时序数据的写入有很大差异：

OLTP事务写入 | 时序数据写入
------------ | -------------
主要是更新 | 主要是插入
数据随机分布 | 热点集中在最新的时间段数据
通常是基于主键的事务操作 | 除了时间戳之外，还关联其他主键比如服务器ID，设备ID，账号ID，设备ID等等

时序数据堆积非常迅速，而通常关系数据库无法很好的扩展，因此，大多数开发人员选择时序数据库倾向于扩展性更好的 NoSQL 方案。

# 关系数据库为什么无法满足时序数据的存取需求？

摘自 TimescaleDB 团队对 PostgreSQL 插入性能测试，可以看出数据插入吞吐量随着表数据量的增加而下降，该测试基于 PostgreSQL 9.6.2 版本，SSD 磁盘，8 vCPU 存储优化云主机，客户端每次插入一行数据，包含12列：时间戳，随机主ID和10个其他指标数据）；PostrgreSQL 开始以 15K每秒的速度插入，但随后在 50M 行后开始明显下降，性能变化差异巨大，有时仅仅 100次每秒插入性能；

![PG插入性能]({{site.image-srv}}/img/20191226/1.gif)

这通常由关系数据库的特性决定的，关系数据库在磁盘上利用数据页来保存数据，比如 8KB大小单位，基于数据页，数据库系统构建数据结构比如 B+树，来快速索引和访问数据，利用索引，SQL 查询比如通过身份证ID 可以快速定位到磁盘上的数据，避免扫描整个数据表；如果整个索引比较小，可以全部放入内存操作，那性能非常好，但如果如果内存无法容纳所有的 B+树，更新树的一个随机部分，可能会导致严重影响性能的磁盘 I/O 操作，数据库系统需要从磁盘读取数据页到内存，在内存中更新，再写回磁盘；因此由于数据库系统以数据页为单位访问磁盘，哪怕很小的数据更新，都可能导致比如 8KB 大小的内存和磁盘的多次数据交换；

新型的 NoSQL 数据库比如LevelDB、Cassandra，InfluxDB 等，为了避免低效的基于磁盘的 B+树索引数据结构（双倍的磁盘 I/O操作），引入了一种新型基于磁盘的 [LSM（Log-Structure Merge）](https://www.cs.umb.edu/~poneil/lsmtree.pdf)树结构，有效降低维护一个实时索引树的代价，提高了数据插入和删除效率，比较适合插入多于查询的场景比如历史记录表，日志表或时序数据； LSM 的优化逻辑是，减少碎片小数据写入操作，仅仅执行较大数据集及磁盘追加写入；与 B+数”就地“写入不一样，LSM 树将最近的更新包括删除操作排入内存队列（Sorted String Table-SSTable），当数据量足够大的时候，批量写回磁盘，这样就降低了磁盘 I/O 的成本；但 LSM 也引入一些弊端：

- 更多内存需求：与 B+树不一样，LSM 树没有一个全局的排序树，因此基于个 Key 值进行一次查询变得复杂，首先，要检查内存中的 SSTable 是否有该 Key的最新值，否则还需要查询磁盘上的数据来定位到最新的 Key对应的数值；为了避免过多的磁盘 I/O 操作，所有的 SSTable的索引需要保存在内存中，这又增加了内存的需求；

- 二级索引不支持：由于缺乏全局顺序，因此 LSM 树结构不支持二级索引；不同的系统做了一些变通，比如冗余复制一份数据但以不同的顺序存储，或者将主键构建为多个值的”串接“，等等；

# 选择一个时序数据库需要关注哪些点？

# NoSQL时序数据库 vs 关系数据库扩展的时序数据库

# 时序数据库选型关注哪些方面？
